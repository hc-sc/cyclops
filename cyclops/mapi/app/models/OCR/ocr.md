# Optical Character Recognition
Before inspecting a product, regulators need to start by finding where the text is in the image and then read it. Only after that can they comprehend the text. Cyclops starts the same way. It needs to find the text and then formulate a sensible word from it. The former is called text detection and the latter is text recognition.
## Text Detection
Based on Character Region Awareness for Text Detection: https://arxiv.org/abs/1904.01941
NHP text varies significantly in shape, size and background. The circular curved text can shy around the edges. If a brand name has a fancy font with a leaf instead of a character o then it should still take the whole word together instead of separating by the leaf. Instead of the leaf, a glare may be hindering a character or two. The model should still assemble the word together. NPN is important to inspectors but not to consumers so it is small and in a corner yet crucial to detect.
After testing many models, Character Region Awareness for Text Detection (CRAFT) proved to predict the presence of text and to localize each word superiorly. It flexibly combines characters by searching for characters and putting them together into a word rather than outlining rigid word-level bounding boxes on arbitrary word regions. Because of its flexibility, it detects curved, slanted and rotated text exceptionally well. Since it looks at the character distances rather than word sizes, it correctly detects very long words or very short words equally well as common length words. The code and pre-trained models are open sourced: https://github.com/clovaai/CRAFT-pytorch
CRAFT’s fully convolutional network outputs the character region score and the affinity score. The former score localizes individual characters, whereas the latter score is the strength to group each character into a single instance. The probability scores are encoded with a Gaussian heat map. Since character-level annotation training data does not exist, CRAFT weakly learns character level ground truth from the word-level dataset. The word level dataset has the number of characters per word, so the model’s prediction confidence will be the number of characters it detected out of the actual number of characters. This confidence is used as the learning weight during the training. 

## Text Recognition
Based on What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis: https://arxiv.org/pdf/1904.01906.pdf 
The target images contain scene text images (unlike scans for example) because the images are taken from different angles and under imperfect conditions. While similar to the target images, our target images contain symbols which are overlooked in the research of Scene Text Recognition (STR). STR models may accurately recognize the alphabets and numbers but not recognize symbols because they are not trained to know the symbols existed. A future step will be to train it with symbols but that requires a great amount of labelled data and effort to fine tune it. Considering that we can leverage the efforts on tesseract, using the STR pre-trained model is more efficient and does not sacrifice notable accuracy.
For alphabets and numbers, we use a STR model but we combine it with tesseract to recognize symbols. Tesseract, however, is not always correct at predicting symbols and the character may have actually been an alphabet instead of a symbol. We assume that tesseract is correct at predicting symbols and place it where there is a difference between the STR prediction and the tesseract prediction.
Many STR models have a similar underlying structure of four main steps but the algorithm used for each step varies. The steps are transformation, feature extraction, sequence modeling, and prediction. The image first transforms into a normalized image where it is less warped. Then the model extracts features that are relevant to recognize the text; for example, the shape is chosen over size and color attributes. The contextual information about the character sequences then supplements the data to predict the words more robustly. Note that this works better if the training language is the same as the test language where sub word information is useful. The final step predicts the sequence of characters based on the image’s attributes. 
We selected the algorithms that collectively produce the best accuracy but an alternate combination of STR may be more applicable to reduce time and/or memory in place of a bit of accuracy. Our pre-trained model in the original paper’s github page: https://github.com/clovaai/deep-text-recognition-benchmark. For example, the Attention model in the last step (Prediction) slows down the process significantly with a small cost in accuracy. This is compared to Connectionist temporal classification. On the other hand, the feature extraction model choice impacts the memory use. Heave feature extractor like ResNet burdens the memory consumption compared to VGG and RCNN. Interesting to note that the changing the feature generation model does not impact the speed much compared to the memory but changing the prediction model also does not impact the memory consumption much as compared to the speed. Multiple pre-trained models exist in the open sourced Deep Text Recognition project so that practitioners can use the appropriate module combination to their needs.
